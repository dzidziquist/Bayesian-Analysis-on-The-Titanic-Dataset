---
title: "Final Project Report"
mainfont: Times New Roman
output:
  pdf_document: default
  html_document: default
sansfont: Times New Roman
---



![](download.png) 

##  _Statistical Methods in Data Science and Laboatory II_

### _Masters in Data Science_

## _Bayesian Analysis on The Titanic Dataset_

### _Awumee Quist Maureen Dzifa_

\pagebreak

# INTRODUCTION

Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. 


Bayes' theorem is a conditional probability where the probability of the occurence of an event is based on prior knowledge of conditions that might be related to the event.
For example; in reference to this particular project, the titanic dataset is used. The variables of this dataset include survived, sex, pclass, fare, Siblings.Spouses.Aboard and Parents.Children.Aboard. Therefore giving an example that probability of survived is conditioned on sex. 

that is : 
$$P(Survived|Sex)$$
This can be calculated with the formula 
$$P(Survived|Sex) = \frac {P(Survived \cap Sex)} {P(Sex)} = \frac {P(Sex|Survived) \ast P(Survived)}{P(Sex)} $$

With Bayesian Inference, the posterior distribution is derived from the prior distribution and the likelihood function derived from a statistical model for the observed data. 

That is;

$$posterior \ distribution = prior \ distribution\  \ast \ likelihood \  function  $$

In this project, the titanic dataset is used. The main aim of this project is to perform a bayesian analysis on the probability of the females and males surviving based on the observed data on the survived variable. 

Also, to perform a bayesian logistic regression based on the sex or gender in order to predict future observations or missing observations.
\pagebreak 

# THE TITANIC DATASET

The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.The titanic dataset is avaliable in R. To explore more with the dataset, the dataset was downloaded and cleaned to remove irrelevant columns and also convert categorical/qualitative data types into quantative/numerical data types.
 

```{r}
#loading the original dataset 
Titanic <- read.csv(file = "titanic.csv", header = TRUE)
head(Titanic)
```


Quantitative form of the dataset, where female is coded 0 and male coded 1. 

```{r}
#loading the data 
dataset <- read.csv(file = "dataset.csv", header = TRUE)
head(dataset)
```

Exploring the dataset:

```{r}
#finding correlation with respect to survived 
correlation = cor(dataset, dataset$Survived)
correlation
```

```{r}
library(corrplot)
corrplot(correlation, method = "pie")
```


It can be seen that Pclass, Sex and Fare have higher correlation to the variable survived as compared to the rest of the variables. 

Therefore keeping these variables:

```{r}
#dropping columns with the minimal correlation
keeps <- c("Survived","Sex", "Pclass", "Fare")
dataset = dataset[keeps]
head(dataset)
```

Exploring the titanic dataset

```{r}
ss <- dataset$Survived   #data on survived 
sx <- dataset$Sex        #data on sex 
```


```{r}
summary(ss)    #summary on survived 
summary(sx)    #summary on sex 
```

It can be seen that although both variables have the minimum and maximum to be zero and one respectively, the median for survived is zero and that of sex is 1. Where zero for the survived variable represent "does not survive" and one for sex represents "male"

```{r}
density <- density(ss)
plot(density, main = "Survived")
polygon(density, col="orchid", border="red")
density <- density(sx)
plot(density, main = "Sex")
polygon(density, col="yellow", border="red")
```
It can be seen that the data points of both variables are concentrated around zero(0) and one(1), hence concluding that, it is a binary dataset.

\pagebreak 


# BAYESIAN APPROACH 

## Building The Statistical Model

let s be survived and g be sex. Using a Bernoulli statistical model. The survival of a person conditioned or given the sex or gender. Therefore, the data on survived is conditioned on the sex or gender of the individual since the correlation between survived and sex is higher as compared to the other variables. 


$${y}_{s/g} \sim bern(\theta_g)$$

Therefore, using the conjugate analysis for bernulloi for the prior distribution of sex or gender.

$$\theta_g \sim beta(\alpha, \beta)$$


The final dataset to perform the bayesian analysis 

```{r}
survived <- Titanic$Survived 
sex <- Titanic$Sex
finaldata <- data.frame(sex, survived ) #getting the data for the survived and sex variables. 
```



```{r}
#order the dataset by sex
finaldata <- finaldata[order(sex),] 
```


## Building The Problem or Hypothesis 


```{r}
library(gmodels)
survival_based_on_sex <- table(survived,sex) # 1=survived, 0=did not surive
Results <- CrossTable(survival_based_on_sex, prop.chisq=FALSE, prop.r = FALSE, prop.t = FALSE)
```


Getting the results of the probabilities of survived and does not survived for both males and females 

```{r}
results <- Results$prop.col
results
```


From the result above, the P(survive|female) = 0.74 whereas the P(survive|male) = 0.19. Therefore, we can state the hypothesis as females are more likely to survive than males

**GOAL**: Females are more likely to survive as compared to males on the posterior belief that the individual is a female given that the individual survives.



### Performing Analysis in Jags explicitly writing down the model in OpenBugs Language

```{r}
library(rjags)
library(R2jags)
library(mcmc)
library(coda)
library(ggmcmc)
```


\pagebreak 

# MODEL 1 : SIMPLE BAYESIAN ANALYSIS 

The data on survived is conditioned on the sex or gender of the individual since the correlation between survived and sex is higher as compared to the other variables.

$${y}_{s/g} \sim bern(\theta_g)$$


Conjugate analysis for bernulloi for the prior distribution of sex or gender.

$$\theta_g \sim beta(\alpha, \beta)$$


```{r}
#writing the model in R(model specification)
model <- "model{
  for (i in 1:Nsur) {
    s[i] ~ dbern(theta[g[i]])
  }
  
    
    #prior 
    for (g in 1:Nsex){
      theta[g] ~ dbeta(3, 2)    #alpha = 3, beta = 2
    }
  
}"
```

Creating data list from the dataset 

```{r}
#creating data list
dl <- list(s =finaldata$survived, g = finaldata$sex, Nsur = length(finaldata$survived), Nsex = length(unique(finaldata$sex)))
```


```{r}
?jags.model
#using jags
# Running the model
model1_fit <- jags.model(textConnection(model), data = dl, n.chains = 3, n.adapt= 1000)
```

#### Peforming a burnin of 1000, sampling 10000 samples per chain 

```{r}
update(model1_fit, 1000); # Burn in 
mcmc_samples <- coda.samples(model = model1_fit, variable.names=c("theta"), n.iter=10000)  #markov chain monte carlo sample
summary(mcmc_samples)     #10000 samples 
```

The Empiricial means of theta 1 and theta 2 are 0.7401 and 0.1937 respectively. Therefore the mean survival rate of females is 0.7401 and that of males is 0.1937. These empiricial means are samples from the posterior distribution.


# DIAGNOSTICS CHECK FOR MODEL 1  

### burnin of 1000, iteration of 10000

```{r}
#checking for convergence(density plot)
mcmc_samples_results = ggs(mcmc_samples)
ggs_density(mcmc_samples_results)
```


```{r}
#Trace Plot 
ggs_traceplot(mcmc_samples_results)
```



```{r}
#Auto correltaion 
ggs_autocorrelation(mcmc_samples_results)
```

#### Peforming a burnin of 10000, sampling 100000 samples per chain 

```{r}
update(model1_fit, 10000); # Burn in for 10000 samples
mcmc_samples2 <- coda.samples(model = model1_fit, variable.names=c("theta"), n.iter= 100000)  #markov chain monte carlo sample    n.iter of 10000
summary(mcmc_samples2)
```
The Empiricial means of theta 1 and theta 2 are 0.7398 and 0.1937 respectively. Therefore the mean survival rate of females is 0.7398 and that of males is 0.1937. These empiricial means are samples from the posterior distribution.


# DIAGNOSTICS CHECK FOR MODEL 1 
### burnin of 10000, iteration of 100000

```{r}
#checking for convergence(posterior density plot)
mcmc_samples2_results = ggs(mcmc_samples2)
ggs_density(mcmc_samples2_results)    #for burn in of 10000 samples 
```

```{r}
ggs_autocorrelation(mcmc_samples2_results)   #for burn in of 10000 samples 
```


```{r}
#traceplot 
ggs_traceplot(mcmc_samples2_results)   #burn in for 10000
```


Comparing the burn in of 1000 with 10000 samples to burn in of 10000 with 100000 samples, the plots above of the burn in of 10000 (this has n.iter of 100000, hence 100000 samples) are more convergence as compared to burn in of 10000.


The probability of Females to survive, building a null hypothesis under the posterior distribution:
Hypothesis: $$ H_{0}:\theta_{g}/y_{s} = 0.7399 \\ H_{1}: \theta_{g}/y_{s} \neq 0.7399$$


That is:
Hypothesis: $$ H_{0}:\theta_{g}/y_{s}  = 0.74 \\ H_{1}: \theta_{g}/y_{s}  \neq 0.74$$ 




Finding the credible interval of the markov chain monte carlo(MCMC) simulated sample

```{r}
library(TeachingDemos)
?HPDinterval
HPDinterval(mcmc_samples, prob = 0.95)
HPDinterval(mcmc_samples2, prob = 0.95)
```

Using an MCMC sample to estimate the posterior density, then the 95% CI is estimated using the 0.025 and 0.975 quantiles of the sampled output of 1000 the number of burn-in iterations is:

$$0.69 \leqslant \theta_{g}/y_{s} \leqslant 0.7869$$


 
 


\pagebreak 


# MODEL 2: BAYESIAN LOGISTIC REGRESSION
## CREATING A LOGISTIC REGRESSION

The purpose of this model is to perform a regression analysis on the given data and also to be able to give future predictions missing observations based on a train dataset.

$$s_{i} \sim ber(\theta_{i})$$

Building a logistic regression model:
$$\theta_{i} = \beta_{1} + \beta_{2}*g_{i}$$

$$\beta_{1} \sim beta(3,2)$$

$$\beta_{2} \sim beta(3,2)$$



```{r}
#creating data list
dlist <- list(s =finaldata$survived, g = dataset$Sex, Nsur = length(finaldata$survived) ,Nsex = length(unique(finaldata$sex)))
```



```{r}
#writing the model in R(model specification)
#building a logistic regression model 
model2 <- "model{
  for (i in 1:Nsur) {
    s[i] ~ dbern(theta[i])
    
    logit(theta[i]) <-  beta[1] + beta[2]*g[i]

  }
   #prior 
 for (j in 1:Nsex){
 beta[j] ~  dbeta(0.5,0.5)
 }
}"
```


    


```{r}
# Running the model
model2_fit <- jags.model(textConnection(model2), data = dlist, n.chains = 3, n.adapt= 10000)
```

#### Peforming a burnin of 1000, sampling 10000 samples per chain 


```{r}
update(model2_fit, 1000); # Burn in for 1000 samples
mcmc_sample <- coda.samples(model = model2_fit, variable.names=c("beta"), n.iter=10000)  #markov chain monte carlo sample
summary(mcmc_sample)
```

The empirical means of the coefficients of $$\beta_{1}$$ and $$\beta_{2}$$ are 0.02607 and 0.03659 respectively. Therefore it can be concluded that $$\beta_{1}$$ is 0.02607 and $$\beta_{2}$$ is 0.03659.

# DIAGNOSTICS CHECK FOR MODEL 2
### burnin of 1000, iteration of 10000

```{r}
#checking for convergence(density plot)
mcmc_sample_results = ggs(mcmc_sample)
ggs_density(mcmc_sample_results)
```


```{r}
#Trace Plot 
ggs_traceplot(mcmc_sample_results)
```


```{r}
#Trace Plot 
ggs_autocorrelation(mcmc_sample_results)
```

#### Peforming a burnin of 10000, sampling 100000 samples per chain 


```{r}
update(model2_fit, 10000); # Burn in for 1000 samples
mcmc_sample2 <- coda.samples(model = model2_fit, variable.names=c("beta"), n.iter=100000)  #markov chain monte carlo sample
summary(mcmc_sample2)
```

The empirical means of the coefficients of $$\beta_{1}$$ and $$\beta_{2}$$ are 0.02602 and 0.03664 respectively. Therefore it can be concluded that $$\beta_{1}$$ is 0.02602 and $$\beta_{2}$$ is 0.03664

# DIAGNOSTICS CHECK FOR MODEL 2
### burnin of 10000, iteration of 100000

```{r}
#checking for convergence(density plot)
mcmc_sample2_results = ggs(mcmc_sample2)
ggs_density(mcmc_sample2_results)
```


```{r}
#Trace Plot 
ggs_traceplot(mcmc_sample2_results)
```


```{r}
#Trace Plot 
ggs_autocorrelation(mcmc_sample2_results)
```

It can also be seen here that comparing the burn in of 1000 with 10000 samples to burn in of 10000 with 100000 samples, the plots above of the burn in of 10000 (this has n.iter of 100000, hence 100000 samples) are more convergence as compared to burn in of 10000.


\pagebreak 

# MODEL 3: BAYESIAN LOGISTIC REGRESSION

## CREATING A LOGISTIC REGRESSION 

$$\theta_{i} = \beta_{1} + \beta_{2} \ \ast \ g_{i} $$

$$s_{i} \sim ber(\theta_{i})$$
$$\beta_{1} ~ N(0,02)$$

$$\beta_{2} ~ N(0,02)$$

```{r}
#creating data list
dlist3 <- list(s =finaldata$survived, g = dataset$Sex, Nsur = length(finaldata$survived) ,Nsex = length(unique(finaldata$sex)))
```


```{r}
model3 <- "model{
  for (i in 1:Nsur) {
    logit(theta[i]) <- beta[1] + beta[2]*g[i] 
    s[i] ~ dbern(theta[i])
  }
  for (j in 1:Nsex){
  beta[j] ~ dnorm(0.0, 0.2) 
  }
}"
```


```{r}
# Running the model
model3_fit <- jags.model(textConnection(model3), data = dlist, n.chains = 3, n.adapt= 10000)
```

#### Peforming a burnin of 1000, sampling 10000 samples per chain 


```{r}
update(model3_fit, 1000); # Burn in for 1000 samples
mcmc_samplee <- coda.samples(model = model3_fit, variable.names=c("beta"), n.iter=10000)  #markov chain monte carlo sample
summary(mcmc_samplee)
```

The empirical means of the coefficients of $$\beta_{1}$$ and $$\beta_{2}$$ are -0.3988 and -0.1045 respectively. Therefore it can be concluded that $$\beta_{1}$$ is -0.3988 and $$\beta_{2}$$ is -0.1045

# DIAGNOSTICS CHECK FOR MODEL 3
### burnin of 1000, iteration of 10000

```{r}
#checking for convergence(density plot)
mcmc_samplee_results = ggs(mcmc_samplee)
ggs_density(mcmc_samplee_results)
```



```{r}
#Trace Plot 
ggs_traceplot(mcmc_samplee_results)
```


```{r}
#Trace Plot 
ggs_autocorrelation(mcmc_samplee_results)
```


#### Peforming a burnin of 10000, sampling 100000 samples per chain 


```{r}
update(model3_fit, 10000); # Burn in for 1000 samples
mcmc_samplee2 <- coda.samples(model = model3_fit, variable.names=c("beta"), n.iter=100000)  #markov chain monte carlo sample
summary(mcmc_samplee2)
```

The empirical means of the coefficients of $$\beta_{1}$$ and $$\beta_{2}$$ are -0.4009 and -0.1022 respectively. Therefore it can be concluded that $$\beta_{1}$$ is -0.4009 and $$\beta_{2}$$ is -0.1022


# DIAGNOSTICS CHECK FOR MODEL 3
### burnin of 10000, iteration of 100000

```{r}
#checking for convergence(density plot)
mcmc_samplee2_results = ggs(mcmc_samplee2)
ggs_density(mcmc_samplee2_results)
```


```{r}
#Trace Plot 
ggs_traceplot(mcmc_samplee2_results)
```


```{r}
#Trace Plot 
ggs_autocorrelation(mcmc_samplee2_results)
```



\pagebreak

# COMPARATIVE ANALYSIS WITH FREQUENTIST INFERENCE 

## MODEL 1

Building the frequentist approach 
Using the parametric bootstrap sampling based on the statistical model 
$${y}_{s/g} \sim bern(\theta_g)$$



```{r}
#dataframe of only females 
female_data <- finaldata[which(finaldata$sex == "female"), ]
```


```{r}
#dataframe of only males 
male_data <- finaldata[which(finaldata$sex == "male"), ]
```


Using the maximum likelihood estimation to find theta, that is probability or chances of females surviving 
```{r}
#Maximum likelihood estimator for theta(chances of female survival) 
theta_hat <- function(x){
  sum(x)/length(x)
}
theta_mle = theta_hat(female_data$survived)
theta_mle
```
probability or chances of females surviving is  0.742


```{r}
#Maximum likelihood estimator for theta(chances of male survival) 
theta_hat_2 <- function(x){
  sum(x)/length(x)
}
theta_mle_2 = theta_hat(male_data$survived)
theta_mle_2
```
probability or chances of male survival is  0.190


```{r}
#Maximum likelihood estimator for theta(chances of survival for both females and males) 
theta_hat_2 <- function(x){
  sum(x)/length(x)
}
theta_mle_3 = theta_hat(finaldata$survived)
theta_mle_3
```

chances of survival for both females and males is 0.3855693

Performing bootstrap sampling and performing the above hypthesis. 

```{r}
library(Rlab)
#estimation for chances of female survival 
# Parametric Boostrap
n <- length(finaldata$sex)        # Sample size
B <- 10000                         # Bootstrap size
theta.boot <- rep(NA, B)              # Init
set.seed(123)
for(b in 1:B){
x.boot <- rbern(n, prob = theta_mle)       #simulated sample 
theta.boot[b] <- theta_hat(x.boot)
}
cat("theta estimate \n")
mean(theta.boot)
# Parametric bootstrap CI
cat('95% level of confidence \n')
round(
c(lower = theta_mle - qnorm(1 - 0.05/2)*sd(theta.boot),
upper = theta_mle + qnorm(1 - 0.05/2)*sd(theta.boot)), 3)
```
Comparing to model 1 of 10000 burn in and 100000 iterations, it can be seen that theta of female is 0.7398 while that of the frequentist is 0.742. 

```{r}
#estimation for chances of male survival 
# Parametric Boostrap
n <- length(finaldata$sex)        # Sample size
M <- 10000                         # Bootstrap size
theta.boot_2 <- rep(NA, M)              # Init
set.seed(123)
for(m in 1:M){
x.boot_2 <- rbern(n, prob = theta_mle_2)       #simulated sample 
theta.boot_2[m] <- theta_hat(x.boot_2)
}
cat("theta estimate \n")
mean(theta.boot_2)
# Parametric bootstrap CI
cat('95% level of confidence \n')
round(
c(lower = theta_mle_2 - qnorm(1 - 0.05/2)*sd(theta.boot_2),
upper = theta_mle_2 + qnorm(1 - 0.05/2)*sd(theta.boot_2)), 3)
```
Comparing to model 1 of 10000 burn in and 100000 iterations, it can be seen that theta of male is 0.1937  while that of the frequentist is 0.19025. 



```{r}
#estimation for chances of survival
# Parametric Boostrap
n <- length(finaldata$sex)        # Sample size
M <- 10000                         # Bootstrap size
theta.boot_3 <- rep(NA, M)              # Init
set.seed(123)
for(m in 1:M){
x.boot_3 <- rbern(n, prob = theta_mle_3)       #simulated sample 
theta.boot_3[m] <- theta_hat(x.boot_3)
}
cat("theta estimate \n")
mean(theta.boot_3)
# Parametric bootstrap CI
cat('95% level of confidence \n')
round(
c(lower = theta_mle_3 - qnorm(1 - 0.05/2)*sd(theta.boot_3),
upper = theta_mle_3 + qnorm(1 - 0.05/2)*sd(theta.boot_3)), 3)
```

Chances of both males and females surviving using the frequentist approach is 0.3856


\pagebreak 

# COMPARATIVE ANALYSIS WITH FREQUENTIST INFERENCE 

## MODEL 2 AND MODEL 3

```{r}
s <- finaldata$survived
g <- dataset$Sex
```


```{r}
glm(s ~ g, family = binomial(link='logit'))
```
This is completely related to model 3 of burn in 10000 and iterations of 100000, where the intercept is -0.4009 and slope  is -0.1022. With the frequentist it can be seen that the intercept is -0.4002 and the slope is -0.1023

\pagebreak 

# MODEL COMPARISON USING DIC 

## MODEL 2 VS MODEL 3 UNDER BAYESIAN ANALYSIS

Deviance Information Criteria(DIC) is a measure of model comparison and adequacy. It is given by the expression:

$$DIC(m) = \overline{2D(\theta_{m}, m)} - D(\bar\theta_{m}: m) = D(\bar\theta_{m}, m) + 2p_{m}$$ ;

where,

$$D(\theta_{m},m) = -21ogf(y|\theta_{m},m)$$

The above equation is the usual deviance measure. 

$$p_{m} = \overline{D(\theta_{m}, m)} - D(\bar\theta_{m}, m)$$
Smaller DIC values indicate a better-fitting model. That is, models with smaller DIC values are supported by the data.

From the above density plots, since the posterior mean is not highly skewd or bimodal, it is safe to use the DIC measure of model comparison and adequacy.


Deviance Information Criteria(DIC) for Model 1 with different burn ins.

```{r}
library(loo)
```

```{r}
model2_dic1 <- dic.samples(model2_fit, n.iter = 1000, thin =1)  #burn in of 1000
model2_dic2 <- dic.samples(model2_fit, n.iter = 10000, thin =1) #burn in of 10000
```

```{r} 
model2_dic1
model2_dic2
```


Difference between model 2 with 1000 burnin, 10000 iteration and model 2 with 10000 burnin, 100000 iteration

```{r}
diffdic(model2_dic1, model2_dic2)
```



Deviance Information Criteria(DIC) for Model 2 with different burn ins.

```{r}

model3_dic1 <- dic.samples(model3_fit, n.iter = 1000, thin =1)  #burn in of 1000
model3_dic2 <- dic.samples(model3_fit, n.iter = 10000, thin =1) #burn in of 10000
```

```{r}
model3_dic1
model3_dic2
```



Difference between model 3 with 1000 burnin, 10000 iteration and model 3 with 10000 burnin, 100000 iteration

```{r}
diffdic(model3_dic1, model3_dic2)
```

Difference between model 2 with 1000 burnin, 10000 iteration and model 3 with 1000 burnin, 10000 iteration

```{r}
diffdic(model2_dic1, model3_dic1)
```


Difference between model 2 with 10000 burnin, 100000 iteration and model 3 with 10000 burnin, 100000 iteration

```{r}
diffdic(model2_dic2, model3_dic2)
```

Comparing model 2 to model 3, model has the smaller DIC, so it can be concluded that model 3 best fits the data as compared to model 3. The difference between these two models is the choice of the prior distributions. The prior distributions of model 2 follows a beta distribution while that of model 3 follows a normal distribution. 

\pagebreak  

# PREDICTION 

In Bayesian theory, predictions of future or missing observations are based on predictive distributions. This is giving in the Bayesian Modeling Using WinBugs book by Ntzoufras. 

The test dataset contains just the sex column. Therefore using that and the model2 built since it is the better fitting model for the dataset, the survived of each of the sex in the test dataset is predicted.

```{r}
#loading in the test dataset
sex <- read.csv(file = "test.csv", header = TRUE)
sex <- sex$Sex
```

```{r}
#creating data list
dlistt <- list(s =finaldata$survived, g = dataset$Sex, Nsur = length(finaldata$survived) ,Nsex = length(unique(finaldata$sex)), N_pred=length(sex), sex = sex)
```


```{r}
#writing the model in R(model specification)
#building a logistic regression model 
model_predict <- "model{
  for (i in 1:Nsur) {
    logit(theta[i]) <-  beta[1] + beta[2]*g[i]
    s[i] ~ dbern(theta[i])
  }
  
   #prior 
 for (j in 1:Nsex){
  beta[j] ~ dnorm(0.0, 0.2) 
 }

 #prediction(survive or does not survive)
 for(j in 1:N_pred) {
    logit(pred_theta[j]) <-  beta[1] + beta[2]*sex[j]
    pred_s[j] ~ dbern(theta[j])
  }
}"
```


```{r}
# Running the model
model_predict_fit <- jags.model(textConnection(model_predict), data = dlistt, n.chains = 1, n.adapt= 10000)
```


```{r}
update(model_predict_fit, 10000); # Burn in for 1000 samples
predict_sample <- coda.samples(model = model_predict_fit, variable.names=c("pred_s"), n.iter=100000)  #markov chain monte carlo sample
```


```{r}
library(data.table) 
predict <- do.call(rbind.data.frame, predict_sample)
```


\pagebreak 

# REFERENCES

1. (Ntzoufras, 2009) I. Ntzoufras, Bayesian Modeling Using WinBugs
2. Introduction to Bayesian Monte Carlo methods in WINBUGS [link](https://www.stat.ubc.ca/~gavin/STEPIBookNewStyle/computing/winbugs/bayes-intro-2007-slides.pdf)
3. Bayesian Inference for Linear and Logistic Regression Parameters [link](http://www.medicine.mcgill.ca/epidemiology/Joseph/courses/EPIB-668/bayesreg.pdf)